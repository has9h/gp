{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "from matplotlib import animation, cm\r\n",
    "from mpl_toolkits.mplot3d import Axes3D\r\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Custom Functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def kernel(X1, X2, l=1.0, sigma_f=1.0):\r\n",
    "    \"\"\"\r\n",
    "    Isotropic squared exponential kernel.\r\n",
    "    \r\n",
    "    Args:\r\n",
    "        X1: Array of m points (m x d).\r\n",
    "        X2: Array of n points (n x d).\r\n",
    "\r\n",
    "    Returns:\r\n",
    "        (m x n) matrix.\r\n",
    "    \"\"\"\r\n",
    "    sqdist = np.sum(X1**2, 1).reshape(-1, 1) + np.sum(X2**2, 1) - 2 * np.dot(X1, X2.T)\r\n",
    "    return sigma_f**2 * np.exp(-0.5 / l**2 * sqdist)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def plot_gp(mu, cov, X, X_train=None, Y_train=None, samples=[]):\r\n",
    "    X = X.ravel()\r\n",
    "    mu = mu.ravel()\r\n",
    "    uncertainty = 1.96 * np.sqrt(np.diag(cov))\r\n",
    "    \r\n",
    "    plt.fill_between(X, mu + uncertainty, mu - uncertainty, alpha=0.1)\r\n",
    "    plt.plot(X, mu, label='Mean')\r\n",
    "    for i, sample in enumerate(samples):\r\n",
    "        plt.plot(X, sample, lw=1, ls='--', label=f'Sample {i+1}')\r\n",
    "    if X_train is not None:\r\n",
    "        plt.plot(X_train, Y_train, 'rx')\r\n",
    "    plt.legend()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prior on 1D Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Finite number of points\r\n",
    "X = np.arange(-5, 5, 0.2).reshape(-1, 1)\r\n",
    "\r\n",
    "# Mean and covariance of the prior\r\n",
    "mu = np.zeros(X.shape)\r\n",
    "cov = kernel(X, X)\r\n",
    "\r\n",
    "# Draw three samples from the prior\r\n",
    "samples = np.random.multivariate_normal(mu.ravel(), cov, 3)\r\n",
    "\r\n",
    "# Plot GP mean, uncertainty region and samples \r\n",
    "# plot_gp(mu, cov, X, samples=samples)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Posterior\n",
    "\n",
    "## Prediction from noise-free training data\n",
    "\n",
    "Given a training dataset with noise-free function values $\\mathbf{f}$ at inputs $\\mathbf{X}$, a GP prior can be converted into a GP posterior $p(\\mathbf{f}^* \\vert \\mathbf{X}^*, \\mathbf{X}, \\mathbf{f})$ which can then be used to make predictions $\\mathbf{f}^*$ at new inputs $\\mathbf{X}^*$. By definition of a GP, the joint distribution of observed values $\\mathbf{f}$ and predictions $\\mathbf{f}^*$ is again a Gaussian which can be partitioned into\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "    \\mathbf{f} \\\\\n",
    "    \\mathbf{f}_*\n",
    "\\end{pmatrix} \\sim \\mathcal{N}\n",
    "\\left(\\boldsymbol{0},\n",
    "\\begin{pmatrix}\n",
    "    \\mathbf{K} & \\mathbf{K}_* \\\\\n",
    "    \\mathbf{K}_*^T & \\mathbf{K}_{**}\n",
    "\\end{pmatrix}\n",
    "\\right)\\tag{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(\\mathbf{f}_* \\lvert \\mathbf{X}_*,\\mathbf{X},\\mathbf{f}) &= \\mathcal{N}(\\mathbf{f}_* \\lvert \\boldsymbol{\\mu}_*, \\boldsymbol{\\Sigma}_*) \\\\\n",
    "\\boldsymbol{\\mu_*} &= \\mathbf{K}_*^T \\mathbf{K}^{-1} \\mathbf{f} \\\\\n",
    "\\boldsymbol{\\Sigma_*} &= \\mathbf{K}_{**} - \\mathbf{K}_*^T \\mathbf{K}^{-1} \\mathbf{K}_*\n",
    "\\end{align*}\n",
    "$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from numpy.linalg import inv\r\n",
    "\r\n",
    "def posterior(X_s, X_train, Y_train, l=1.0, sigma_f=1.0, sigma_y=1e-8):\r\n",
    "    \"\"\"\r\n",
    "    Computes the suffifient statistics of the posterior distribution \r\n",
    "    from m training data X_train and Y_train and n new inputs X_s.\r\n",
    "    \r\n",
    "    Args:\r\n",
    "        X_s: New input locations (n x d).\r\n",
    "        X_train: Training locations (m x d).\r\n",
    "        Y_train: Training targets (m x 1).\r\n",
    "        l: Kernel length parameter.\r\n",
    "        sigma_f: Kernel vertical variation parameter.\r\n",
    "        sigma_y: Noise parameter.\r\n",
    "    \r\n",
    "    Returns:\r\n",
    "        Posterior mean vector (n x d) and covariance matrix (n x n).\r\n",
    "    \"\"\"\r\n",
    "    K = kernel(X_train, X_train, l, sigma_f) + sigma_y**2 * np.eye(len(X_train))\r\n",
    "    K_s = kernel(X_train, X_s, l, sigma_f)\r\n",
    "    K_ss = kernel(X_s, X_s, l, sigma_f) + 1e-8 * np.eye(len(X_s))\r\n",
    "    K_inv = inv(K)\r\n",
    "    \r\n",
    "    # Equation (7)\r\n",
    "    mu_s = K_s.T.dot(K_inv).dot(Y_train)\r\n",
    "\r\n",
    "    # Equation (8)\r\n",
    "    cov_s = K_ss - K_s.T.dot(K_inv).dot(K_s)\r\n",
    "    \r\n",
    "    return mu_s, cov_s\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1D Train Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Noise free training data\r\n",
    "X_train = np.array([-4, -3, -2, -1, 1]).reshape(-1, 1)\r\n",
    "Y_train = np.sin(X_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Compute mean and covariance of the posterior distribution\r\n",
    "mu_s, cov_s = posterior(X, X_train, Y_train)\r\n",
    "\r\n",
    "samples = np.random.multivariate_normal(mu_s.ravel(), cov_s, 3)\r\n",
    "# plot_gp(mu_s, cov_s, X, X_train=X_train, Y_train=Y_train, samples=samples)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1D Prediction from noisy training data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "noise = 0.4\r\n",
    "\r\n",
    "# Noisy training data\r\n",
    "X_train = np.arange(-3, 4, 1).reshape(-1, 1)\r\n",
    "Y_train = np.sin(X_train) + noise * np.random.randn(*X_train.shape)\r\n",
    "\r\n",
    "# Compute mean and covariance of the posterior distribution\r\n",
    "mu_s, cov_s = posterior(X, X_train, Y_train, sigma_y=noise)\r\n",
    "\r\n",
    "samples = np.random.multivariate_normal(mu_s.ravel(), cov_s, 3)\r\n",
    "plot_gp(mu_s, cov_s, X, X_train=X_train, Y_train=Y_train, samples=samples)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Optimization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Optimal values for the kernel parameters $l$ and $\\sigma_f$, as well as the noise parameter $\\sigma_y$ can be estimated by maximizing the log marginal likelihood which is given by\n",
    "\n",
    "$$\n",
    "\\log p(\\mathbf{y} \\lvert \\mathbf{X}) = \n",
    "\\log \\mathcal{N}(\\mathbf{y} \\lvert \\boldsymbol{0}, \\mathbf{K}_y) =\n",
    "-\\frac{1}{2} \\mathbf{y}^T \\mathbf{K}_y^{-1} \\mathbf{y} \n",
    "-\\frac{1}{2} \\log \\begin{vmatrix}\\mathbf{K}_y\\end{vmatrix} \n",
    "-\\frac{N}{2} \\log(2\\pi)\n",
    "$$\n",
    "\n",
    "We will minimize the negative log marginal likelihood w.r.t. parameters $l$ and $\\sigma_f$. $\\sigma_y$ is set to the known noise level of the data. If the noise level is unknown, σy can be estimated as well along with the other parameters."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from numpy.linalg import cholesky, det\r\n",
    "from scipy.linalg import solve_triangular\r\n",
    "from scipy.optimize import minimize\r\n",
    "\r\n",
    "def nll_fn(X_train, Y_train, noise, naive=True):\r\n",
    "    \"\"\"\r\n",
    "    Returns a function that computes the negative log marginal\r\n",
    "    likelihood for training data X_train and Y_train and given\r\n",
    "    noise level.\r\n",
    "\r\n",
    "    Args:\r\n",
    "        X_train: training locations (m x d).\r\n",
    "        Y_train: training targets (m x 1).\r\n",
    "        noise: known noise level of Y_train.\r\n",
    "        naive: if True use a naive implementation of Eq. (11), if\r\n",
    "               False use a numerically more stable implementation.\r\n",
    "\r\n",
    "    Returns:\r\n",
    "        Minimization objective.\r\n",
    "    \"\"\"\r\n",
    "    \r\n",
    "    Y_train = Y_train.ravel()\r\n",
    "    \r\n",
    "    def nll_naive(theta):\r\n",
    "        # Naive implementation of the log marginal likelihood. Works well \r\n",
    "        # for the examples in this article but is numerically less stable \r\n",
    "        # compared to the implementation in nll_stable below.\r\n",
    "        K = kernel(X_train, X_train, l=theta[0], sigma_f=theta[1]) + \\\r\n",
    "            noise**2 * np.eye(len(X_train))\r\n",
    "        return 0.5 * np.log(det(K)) + \\\r\n",
    "               0.5 * Y_train.dot(inv(K).dot(Y_train)) + \\\r\n",
    "               0.5 * len(X_train) * np.log(2*np.pi)\r\n",
    "        \r\n",
    "    def nll_stable(theta):\r\n",
    "        # Numerically more stable implementation of the log marginal likelihood \r\n",
    "        # as described in http://www.gaussianprocess.org/gpml/chapters/RW2.pdf, \r\n",
    "        # Section 2.2, Algorithm 2.1.\r\n",
    "        \r\n",
    "        K = kernel(X_train, X_train, l=theta[0], sigma_f=theta[1]) + \\\r\n",
    "            noise**2 * np.eye(len(X_train))\r\n",
    "        L = cholesky(K)\r\n",
    "        \r\n",
    "        S1 = solve_triangular(L, Y_train, lower=True)\r\n",
    "        S2 = solve_triangular(L.T, S1, lower=False)\r\n",
    "        \r\n",
    "        return np.sum(np.log(np.diagonal(L))) + \\\r\n",
    "               0.5 * Y_train.dot(S2) + \\\r\n",
    "               0.5 * len(X_train) * np.log(2*np.pi)\r\n",
    "\r\n",
    "    if naive:\r\n",
    "        return nll_naive\r\n",
    "    else:\r\n",
    "        return nll_stable\r\n",
    "\r\n",
    "# Minimize the negative log-likelihood w.r.t. parameters l and sigma_f.\r\n",
    "# We should actually run the minimization several times with different\r\n",
    "# initializations to avoid local minima but this is skipped here for\r\n",
    "# simplicity.\r\n",
    "res = minimize(nll_fn(X_train, Y_train, noise), [1, 1], \r\n",
    "               bounds=((1e-5, None), (1e-5, None)),\r\n",
    "               method='L-BFGS-B')\r\n",
    "\r\n",
    "# Store the optimization results in global variables so that we can\r\n",
    "# compare it later with the results from other implementations.\r\n",
    "l_opt, sigma_f_opt = res.x\r\n",
    "\r\n",
    "# Compute posterior mean and covariance with optimized kernel parameters and plot the results\r\n",
    "mu_s, cov_s = posterior(X, X_train, Y_train, l=l_opt, sigma_f=sigma_f_opt, sigma_y=noise)\r\n",
    "# plot_gp(mu_s, cov_s, X, X_train=X_train, Y_train=Y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3D Space"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "rx, ry = np.arange(-5, 5, 0.3), np.arange(-5, 5, 0.3)\r\n",
    "gx, gy = np.meshgrid(rx, rx)\r\n",
    "\r\n",
    "noise_2D = 0.1\r\n",
    "\r\n",
    "X_2D_train = np.random.uniform(-4, 4, (100, 2))\r\n",
    "Y_2D_train = np.sin(0.5 * np.linalg.norm(X_2D_train, axis=1)) + \\\r\n",
    "             noise_2D * np.random.randn(len(X_2D_train))\r\n",
    "Y_2D_train"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generate Meson data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "q_content = np.random.randint(2, size=(10))\r\n",
    "isospin = np.linspace(0, 1.5, 4) \r\n",
    "ang_momentum = np.arange(0, 4.5, 0.5) \r\n",
    "parity = np.array([-1, 1])\r\n",
    "\r\n",
    "# Create new data\r\n",
    "def choose(quantity):\r\n",
    "    return np.array(np.random.choice(quantity))\r\n",
    "\r\n",
    "X = np.empty([50, 13])\r\n",
    "\r\n",
    "for i in range(50):\r\n",
    "    X[i] = np.append(q_content, (choose(isospin), choose(ang_momentum), choose(parity)))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using `sklearn`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For Gaussian processes, we need to add an additional value to distinguish between different particles that constitute the same vector, ranked in order of their masses. From the paper, we will also be using the squared exponential (SE) and rational quadratic (RQ) kernels."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from sklearn.preprocessing import StandardScaler\r\n",
    "from sklearn.metrics import mean_absolute_error\r\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\r\n",
    "from sklearn.model_selection import LeaveOneOut, train_test_split\r\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel, RBF, RationalQuadratic"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Mesons"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Question: How do you handle K-short and K-long?*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Without the following unknown quark contents:\r\n",
    "# f2, a2(1320), π2, η2(1645), a2(1700), η2(1870) \r\n",
    "X = np.array([[1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, -1, 0],\r\n",
    "              [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, -1, 0],\r\n",
    "              [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, -1, 0],\r\n",
    "              [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, -1, 0], # η\r\n",
    "              [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, -1, 1], # η'\r\n",
    "              [1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, -1, 0],\r\n",
    "              [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, -1, 0],\r\n",
    "              [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, -1, 0],\r\n",
    "              [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0],\r\n",
    "              [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, -1, 0],\r\n",
    "              [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0.5, 0, -1, 0],\r\n",
    "              [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0.5, 0, -1, 0],\r\n",
    "              [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0.5, 0, -1, 0],\r\n",
    "              [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0.5, 0, -1, 0],\r\n",
    "              [0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0.5, 0, -1, 0], #K-short\r\n",
    "              [0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0.5, 0, -1, 0], #K-long\r\n",
    "              [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0.5, 1, -1, 0],\r\n",
    "              [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0.5, 1, -1, 0],\r\n",
    "              [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0.5, 1, -1, 0],\r\n",
    "              [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0.5, 1, -1, 0]])\r\n",
    "\r\n",
    "Y = np.array([139.57039, 139.57039, 134.9768, 547.862, 957.78, 775.4, 775.4, 775.49, 782.65, 1019.461,\r\n",
    "              493.677, 493.677, 497.611, 497.611, 497.614, 497.614, 891.66, 891.66, 895.55, 895.55])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# With all particles\r\n",
    "# Unknown quark contents set to 0\r\n",
    "X = np.array([[1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, -1, 0],\r\n",
    "              [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, -1, 0],\r\n",
    "              [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, -1, 0],\r\n",
    "              [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, -1, 0], # η\r\n",
    "              [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, -1, 1], # η'\r\n",
    "              [1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, -1, 0],\r\n",
    "              [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, -1, 0],\r\n",
    "              [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, -1, 0],\r\n",
    "              [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, -1, 0],\r\n",
    "              [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, -1, 0],\r\n",
    "              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0],     # f2\r\n",
    "              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 0],     # a2(1320)\r\n",
    "              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, -1, 0],    # π2\r\n",
    "              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, -1, 0],    # η2(1645)\r\n",
    "              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 1],     # a2(1700) \r\n",
    "              [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, -1, 1],    # η2(1870)\r\n",
    "              [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0.5, 0, -1, 0],\r\n",
    "              [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0.5, 0, -1, 0],\r\n",
    "              [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0.5, 0, -1, 0],\r\n",
    "              [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0.5, 0, -1, 0],\r\n",
    "              [0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0.5, 0, -1, 0], # K-short\r\n",
    "              [0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0.5, 0, -1, 0], # K-long\r\n",
    "              [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0.5, 1, -1, 0],\r\n",
    "              [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0.5, 1, -1, 0],\r\n",
    "              [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0.5, 1, -1, 0],\r\n",
    "              [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0.5, 1, -1, 0]])\r\n",
    "\r\n",
    "Y = np.array([139.57039, 139.57039, 134.9768, 547.862, 957.78, 775.4, 775.4, 775.49, 782.65, 1019.461,\r\n",
    "              1275.5, 1316.9, 1670.6, 1617.0, 1705.0, 1842, # unknown quark contents\r\n",
    "              493.677, 493.677, 497.611, 497.611, 497.614, 497.614, 891.66, 891.66, 895.55, 895.55])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Normalize log masses to have 0 mean and unit variance\n",
    "scaler = StandardScaler()\n",
    "Y_log = np.log(Y)\n",
    "\n",
    "# Not needed if passing `normalize_y=True` when constructing GP object\n",
    "Y_normalized = scaler.fit_transform(Y_log.reshape(-1, 1)).ravel()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Kernels"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Recall the covariance function\n",
    "\n",
    "$$\\mathrm{cov}(\\mathbf{x}_i,\\mathbf{x}_j) = \\kappa(\\mathbf{x}_i,\\mathbf{x}_j) + N_{i,j} \\;, \\text{where} \\; N_{i,j} = \\sigma_y^2 \\delta_{i,j}$$\n",
    "\n",
    "where the squared exponential can be written as\n",
    "\n",
    "$$\n",
    "K_{\\textrm{SE}}(\\mathbf{x}_i,\\mathbf{x}_j) =\n",
    "\\sigma_f^2 \\exp\\left(-\\frac{1}{2l^2}\n",
    "  (\\mathbf{x}_i - \\mathbf{x}_j)^T\n",
    "  (\\mathbf{x}_i - \\mathbf{x}_j)\\right)\n",
    "$$\n",
    "\n",
    "and the rational quadratic kernel is\n",
    "\n",
    "$$\n",
    "K_{\\textrm{RQ}}(\\mathbf{x}_i,\\mathbf{x}_j) =\n",
    "\\sigma_f^2 \\exp\\left(1 + \\frac{1}{2 \\alpha}\n",
    "  (\\mathbf{x}_i - \\mathbf{x}_j)^T\n",
    "  (\\mathbf{x}_i - \\mathbf{x}_j)\\right)^{-\\alpha}\n",
    "$$\n",
    "\n",
    "The prior's covariance is specified by the kernel function. The prior mean is assumed to be data's mean if `normalize_y=True`is passed, and set to constant and $0$ otherwise. \n",
    "\n",
    "Hyperparameters of the kernel are optimized during fitting by maximizing the log-marginal-likelihood, based on the passed optimizer. Optimizer can be started repeatedly by specifying the `n_restarts_optimizer` parameter, as it may have multiple local optima.\n",
    "\n",
    "Starting from the initial hyperparameter values of the kernel, subsequent runs are conducted from hyperparameter values that have been chosen randomly from the range of allowed values.\n",
    "\n",
    "The noise level in the targets can be specified by passing it via the parameter `alpha`, either globally as a scalar or per datapoint. This is added to the diagonal of the kernel matrix during fitting. It can also be interpreted as the variance of additional Gaussian measurement noise on the training observations.\n",
    "\n",
    "The `RBF` kernel only has an `l` parameter which corresponds to the `length_scale` parameter. To have a $\\sigma_f$ parameter as well, we have to compose the `RBF` kernel with a `ConstantKernel`. `l` can either be a scalar (isotropic variant of the kernel) or a vector with the same number of dimensions as the inputs X (anisotropic variant of the kernel).\n",
    "\n",
    "The `RationalQuadratic` kernel is parameterized by a length scale parameter $l > 0$ and a scale mixture parameter $\\alpha > 0$. Only the isotropic variant where `length_scale` is a scalar is supported by `sklearn`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Reshape Y_log or Y_normalized to (-1, 1) if adding noise\n",
    "Y_log = Y_log.reshape(-1, 1)\n",
    "Y_normalized = Y_normalized.reshape(-1, 1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "np.mean(Y_log), np.std(Y_log)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(6.296347305073987, 0.6286352637696876)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "noise = 0.4\n",
    "rbf = ConstantKernel(1.0) * RBF(length_scale=1.0)\n",
    "# rq = ConstantKernel(1.0) * RationalQuadratic(length_scale=1.0, alpha=1.0)\n",
    "\n",
    "# Set GP prior to training data's mean and cov with kernel\n",
    "gpr = GaussianProcessRegressor(kernel=rbf, alpha=noise**2)#, normalize_y=True)\n",
    "# gpr = GaussianProcessRegressor(kernel=rq, alpha=noise**2, normalize_y=True)\n",
    "\n",
    "# Add noise if needed\n",
    "# Y_noisy = Y_log + noise * np.random.randn(*X.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# 80-20 train-test split\n",
    "# Replace the Y's with Y_log, Y_normalized, or Y_noisy as needed\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y_log, test_size=0.2, shuffle=True)\n",
    "print('X Train: ', X_train.shape, 'X Test: ', X_test.shape, 'Y_train: ', Y_train.shape, 'Y_test: ', Y_test.shape)\n",
    "print('Y_test: ', Y_test)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X Train:  (16, 14) X Test:  (4, 14) Y_train:  (16, 1) Y_test:  (4, 1)\n",
      "Y_test:  [[6.92702934]\n",
      " [6.20188146]\n",
      " [6.65337903]\n",
      " [6.86461811]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "np.exp(Y_test[0])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1019.461])"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# Test: Convert changing normalized log value back to MeV scale\n",
    "np.exp((-0.13763566 * np.std(Y_log)) + np.mean(Y_log))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "497.61399978520706"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import numpy as np\n",
    "np.exp(6.24880817)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "517.3958092053244"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# Reuse training data\n",
    "gpr.fit(X_train, Y_train)\n",
    "\n",
    "# Compute posterior mean and covariance\n",
    "# Y_star\n",
    "mu_s, cov_s = gpr.predict(X_test, return_std=True)\n",
    "print(mu_s, cov_s, mu_s.shape, cov_s.shape)\n",
    "\n",
    "# Obtain optimized kernel parameters\n",
    "l = gpr.kernel_.k2.get_params()['length_scale']\n",
    "sigma_f = np.sqrt(gpr.kernel_.k1.get_params()['constant_value'])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[6.20339154]\n",
      " [6.20339071]\n",
      " [6.20339079]\n",
      " [6.20339074]] [0.09998725 0.09998722 0.09998721 0.09998732] (4, 1) (4,)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "np.exp(mu_s[0])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([494.42305699])"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "np.exp((-0.13763566 * np.std(Y_log)) + np.mean(Y_log))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "497.61399978520706"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# Leave-one-out\n",
    "loo = LeaveOneOut()\n",
    "n_splits = loo.get_n_splits(X)\n",
    "\n",
    "l = []\n",
    "mu = []\n",
    "cov = []\n",
    "sigma_f = []\n",
    "for train_index, test_index in loo.split(X):\n",
    "#     print('Train index: ', train_index, 'Test index: ', test_index)\n",
    "    X_star, X_train = X[test_index], X[train_index]\n",
    "    \n",
    "    # Replace the Y's with Y_log, Y_normalized, or Y_noisy as needed\n",
    "    Y_star, Y_train = Y_log[test_index], Y_log[train_index]\n",
    "    \n",
    "    # Reuse training data\n",
    "    gpr.fit(X_train, Y_train)\n",
    "\n",
    "    # Compute posterior mean and covariance\n",
    "    # Y_star\n",
    "    mu_s, cov_s = gpr.predict(X_star, return_std=True)\n",
    "    mu.append(mu_s)\n",
    "    cov.append(cov_s)\n",
    "    \n",
    "    print('log y_star: ', Y_star)\n",
    "    print('Predicted y_star: ', mu_s)\n",
    "    print('Predicted std: ', cov_s)\n",
    "    print('Test index: ', test_index)\n",
    "    \n",
    "    # Obtain optimized kernel parameters\n",
    "    l.append(gpr.kernel_.k2.get_params()['length_scale'])\n",
    "    sigma_f.append(np.sqrt(gpr.kernel_.k1.get_params()['constant_value']))\n",
    "    break\n",
    "\n",
    "# Plot the results\n",
    "# plot_gp(mu_s, cov_s, X, X_train=X_train, Y_train=Y_train)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "log y_star:  [[4.93856906]]\n",
      "Predicted y_star:  [[5.76014688]]\n",
      "Predicted std:  [0.32051062]\n",
      "Test index:  [0]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "mu_s, cov_s"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([[-0.85295951]]), array([0.50985148]))"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "(mu_s * cov_s) + mu_s"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[-1.28784218]])"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "np.exp(-0.85295951 + cov_s**2/2)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.48530135])"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}